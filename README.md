# Turn Based Strategy

Проект был разработан для магистерской диссертации на тему «Разработка игры в жанре пошаговой стратегии с применением нейронных сетей». Как следует из названия, работа была сосредоточена на исследовании возможности использования нейронной сети в роли решателя для компьютерного оппонента в пошаговых стратегиях. Для демонстрации нейронной сети также необходима сама пошаговая стратегия. Чтобы приблизить проект к реальным условиям, где разработчики используют тот или иной игровой движок, игра была разработана на Unreal Engine 4 версии 4.23.1 с использованием языка Blueprints.

Разработанная игра ведётся на прямоугольном поле, состоящем из ячеек, в каждой из которых хранится информация находится ли в ячейке юнит того или иного игрока. В процессе игры игроки перемещают по полю своих юнитов, которые могут атаковать друг друга. Игрок проигрывает и удаляется из игры, когда теряет всех юнитов. Цель игры заключается в том, чтобы остаться единственным игроком.

В ходе работы наиболее подходящим типом машинного обучения, исходя из приведённого выше описания, было определено обучение с подкреплением. Среди методов обучения с подкреплением был выбран алгоритм DQN, который был реализован на языке Python с использованием библиотек tensorflow и numpy. Разработанный модуль был интегрирован в пошаговую стратегию посредством плагина [TensorFlow-Unreal](https://github.com/getnamo/TensorFlow-Unreal) [версии 0.14.0](https://github.com/getnamo/TensorFlow-Unreal/releases/tag/0.14.0).
**Этот модуль можно использовать отдельно от разработанной игры.**

В процессе разработки возникало множество трудностей, связанных с интеграцией модуля, написанного на языке Python, в программу. В связи с этим была разработана [другая версия](https://github.com/lilac-bud/TBSUE4CPP) этого проекта, где алгоритм DQN был реализован в виде библиотеки C++ (которая в свою очередь была разработана с помощью библиотеки xtensor) и сама игра также была переписана на C++. Для второй версии нет необходимости устанавливать сторонний плагин.

<!--TOC-->
  - [1. Описание метода обучения](#learning)
    - [1.1 Глубокая Q-сеть](#dqn)
    - [1.2 Приоритетный выбор переходов](#priority)
    - [1.3 Свёрточная сеть](#cnn)
  - [2. Реализация](#realisation)
    - [2.1 Модуль network](#network)
    - [2.2 AIPawn](#aipawn)
  - [3. Использование приложения](#use)
<!--/TOC-->

<a name="learning"></a>
## 1. Описание метода обучения
<a name="dqn"></a>
### 1.1 Глубокая Q-сеть

В ходе работы была использована глубокая Q-сеть (deep Q-network, DQN), в основе которой лежит Q-обучение. Далее приводится объяснение реализованного алгоритма.

Характеристика подхода:
* Вместо функции $Q(s,a)$, применяемой в Q-обучении, используется функция $Q(s,a;\theta)$, где $\theta$ это параметры (веса сети), которые и будут обучаться.
* Состояния $s$ и действия $a$ представляются в виде характеристических признаков, которые подаются на вход этой функции.
* Выход функции $Q$ это вещественное число – ожидаемый выигрыш или его вероятность в случае бинарного исхода.
* Входом для обучения является случайный мини-батч переходов $(s_t,a_t,r_t,s_{t+1})$ = (состояние, действие, награда, следующее состояние).
* При этом сеть на самом деле представляет из себя две сети одной архитектуры, но одна из них отстаёт от другой. В целевой функции используется именно эта «догоняющая» сеть. Периодически веса должны обновляться.

Таким образом, исходя из всего вышесказанного, алгоритм для шага обучения t можно представить следующим образом:
* Для состояния $s_t$ выбирается действие $a_t$. С вероятностью $\epsilon$ оно случайно, иначе $a_t={\text{argmax}}⁡Q(s_t,a;\theta)$.
* Это действие выполняется, в ответ приходят $r_t$ и $s_{t+1}$. Переход $(s_t,a_t,r_t,s_{t+1})$ записывается в историю.
* Выбирается случайный мини-батч переходов $(s_j,a_j,r_j,s_{j+1})$.
* Функция ошибки считается как $L=\big(y_j-Q(s_j,a_j;\theta)\big)^2$. Для неё делается шаг градиентного спуска, т.е. веса сдвигаются на:
  
$$\nabla_\theta L=2\big(y_j-Q(s_j,a_j;\theta)\big) \nabla_\theta Q(s,a;\theta)$$
  
* В уравнении выше

$$y_j = \begin{cases}
r_j & \text{если эпизод обучения закончился} \\
r_j+\gamma\ \underset{a'}{\text{max}}\ Q(s_{j+1},a';\theta_0 ) & \text{если нет,}
\end{cases}$$
  
&emsp;&emsp;где $\theta_0$ это фиксированные веса второй сети, а $\gamma$ это параметр уценки.

![1_dqn](https://github.com/user-attachments/assets/32c7b30c-2b8f-4f61-8cfb-305c6b3d2a41)

Наконец, к алгоритму применяется подход dueling network. Q-сеть разделена на два канала: один вычисляет функцию позиции $V(s;\theta)$, не зависящую от текущего действия, а другой – зависящую от действия функцию преимущества $A(s,a;\theta)$. Таким образом, Q-значение вычисляется следующим образом:

$$Q(s,a;\theta)=V(s;\theta)+A(s,a;\theta)$$

<img width="440" height="436" alt="2_dueling_network" src="https://github.com/user-attachments/assets/ab2eb306-f753-4c6d-903c-b0c9b0c2f183" />

Стоит заметить, что хотя на практике обычно применяют архитектуру, в которой действие a не подаётся на вход, а у сети столько выходов, сколько возможных действий, в данной работе это невозможно по той причине, что список действий генерируется за пределами нейронной сети в самой игре и варьируется для каждого отдельного состояния.

<a name="priority"></a>
### 1.2 Приоритетный выбор переходов

Как уже было сказано, в процессе обучения из текущей истории игры выбирается несколько случайных переходов. Важнейшей переменной при этом является награда (как следует из принципа обучения с подкреплением). Однако в большинстве случаев $r$ равняется нулю и изменяет своё значение только в конце игры. Следовательно, с большей вероятностью будут выбраны переходы, не предоставляющие полезной информации.

Предпочтительнее использование приоритетности, что обеспечивает совершение выбора, при котором минимизируется ошибка. Для этого нужно посчитать вероятности. В данной работе будет использована следующая формула:

$$P(i)=\frac{p_i^α}{\sum_kp_k^α},$$

где $p_i>0$ это приоритет $i$-того перехода, а $\alpha$ обозначает степень использования приоритетности, $\alpha=0$ соответствует стандартному случайному выбору.

В свою очередь $p_i=|\delta_i+\epsilon|$, где $\delta_i$ это ошибка для $i$-того перехода, а $\epsilon$ это малая положительная константа, которая позволяет избежать случая, когда переходы больше не выбираются для обучения после того, как их ошибка была сведена к нулю.

Применение приоритизации приводит к возникновению смещения, которое нужно скорректировать с помощью весов, которые рассчитываются по следующей формуле:

$$w_i=\left(\frac{1}{N}*\frac{1}{P(i)}\right)^\beta,$$

где $N$ это размер истории переходов. При $\beta=1$ это уравнение полностью компенсирует смещение. Предполагается последовательное увеличение степени $\beta$ с начального значения $\beta_0$ до максимального значения $1$. Кроме того, эти веса нормализируются делением на $\text{max}_i w_i$ для того, чтобы они только уменьшали обновление.

Таким образом, помимо описанной выше приоритизации переходов, в алгоритм также включаются веса, которые умножаются на подсчитанную ошибку. Полная версия алгоритма представлена ниже.

<img width="949" height="552" alt="3_dqn_prioritization" src="https://github.com/user-attachments/assets/4fcf82ce-195a-4fdd-a99b-d4ea496b5b3c" />

<a name="cnn"></a>
### 1.3 Свёрточная сеть
В имплементациях алгоритма DQN в играх обычно применяется предобработка изображения посредством нескольких стандартных слоёв свёрточной сети. Так как в этой работе нет необходимости в обработке изображений, в предыдущей версии такая архитектура была отвергнута. Однако в жанре стратегий важно, как именно элементы на доске располагаются относительно друг друга, и хотелось бы, чтобы это учитывалось при оценке текущего состояния игры. Также в предыдущей версии сеть могла эффективно обучаться только на полях небольшого размера. Кроме того, были найдены научные работы, применяющие свёрточные сети на данных представления игрового поля, не являющегося изображением (в настольной стратегической игре «го»). В связи с этими причинами было решено провести комбинацию алгоритма DQN cо свёрточной сетью.

<img width="1024" height="427" alt="4_cnn" src="https://github.com/user-attachments/assets/57a6cab5-e7d4-4443-9ef8-98a3c3471b02" />

Как показано выше, после прохождения через несколько стандартных слоёв полученные данные объединяются и передаются на обычную полносвязную нейронную сеть, которая так же может иметь несколько слоёв. Такая сеть обучается с помощью метода градиентного спуска.
Новая сеть имеет следующие особенности:
* Вместо изображения на вход подаётся многомерный массив, являющийся представлением состояния игрового поля или возможного действия. Каждый канал соответствует отдельному виду передаваемой информации.
* Так как в работе используется уже упомянутый подход dueling network и так как на вход помимо состояния так же подаётся и действие, свёрточные слои и слои субдискретизации так же разделяются на два канала, один для состояния, другой для действия.
* После прохождения через все свёрточные слои результаты передаются в DQN. Таким образом Q-сеть заменяет собою полносвязные слои свёрточной сети.

И свёрточная сеть, и глубокая Q-сеть обучаются с помощью градиентного спуска, при этом функция ошибки всё ещё считается как было описано ранее. В связи с этим само обучение не претерпит значительных изменений.

<a name="realisation"></a>
## 2. Реализация
<a name="network"></a>
### 2.1 Модуль network
Модуль network написан с использованием библиотек random, tensorflow и numpy. Также импортированы isfile и библиотека json для сохранения и [unreal_engine и TFPluginAPI](https://github.com/getnamo/TensorFlow-Unreal?tab=readme-ov-file#python-api) для интеграции с игрой. Модуль содержит два класса: DuelingModel и Q.

Функция getApi относится ко всему модулю и служит для передачи Q в TensorFlowComponent.
```python
def getApi():
	return Q.getInstance()
```
Класс DuelingModel является подклассом tf.keras.Model и представляет собой реализацию модели, описанной выше. Инициализация зависит от размеров игрового поля и количества каналов.

Класс Q является реализацией всего алгоритма DQN и наследует от класса TFPluginAPI в целях интеграции с игрой. Как было указано ранее, объекты этого класса передаются в игру. В классе используются следующие константы:
* _ALPHA – скорость обучения.
* _GAMMA – параметр уценки.
* _BETA_MIN – минимальное значение $\beta$.
* _PRIORITY_SCALE – степень использования приоритетности.
* _UPDATE_TARGET – периодичность обновления целевой модели.
* _TRAIN – периодичность запуска обучения модели на мини-батче.
* _BATCH_SIZE – размер мини-батча.
* _MIN_TRACE – минимальный размер истории переходов.
* _MAX_TRACE – максимальный размер истории переходов.
* _MIN_EPS – минимальное значение $\epsilon$.
* _MAX_EPS – максимальное значение $\epsilon$.
* _EPS_DECR – скорость уменьшения $\epsilon$.
* _BETA_INCR – скорость увеличения $\beta$.
* _FIELD_HEIGHT – высота игрового поля.
* _FIELD_WIDTH – ширина игрового поля.
* _CHANNELS – количество каналов.

Как можно заметить, одним из существенных недостатков этого проекта является необходимость указания параметров игрового поля непосредственно в модуле network, хотя логика подсказывает, что эти параметры должны передаваться в модуль из игры, где они и определяются (что было реализовано во [второй версии](https://github.com/lilac-bud/TBSUE4CPP) проекта).

Для обеспечения взаимодействия с игрой посредством плагина в классе Q определены несколько специальных методов, которые вызываются извне. Два из них, onSetup и onJsonInput, относятся к опциональным методам, определение которых ожидается плагином (“expected optional api”, см. [TensorFlow-Unreal](https://github.com/getnamo/TensorFlow-Unreal?tab=readme-ov-file#python-api)).

Метод onSetup в данной работе используется для инициализации переменных класса Q.

Метод onJsonInput принимает на вход данные в формате Json и с помощью метода parse_json_input выделяет из них награду, состояние и возможные действия. Если последнее является пустым списком, то считается, что игра окончена, и в этом случае вызывается update. Иначе, вызывается метод get_act, результат которого передаётся в игру.

Методы softReset и saveModel принимают на вход jsonInput, который никак не обрабатывается и присутствует только для того, чтобы их можно было вызвать из игры.
```python
def softReset(self, jsonInput):
    self.soft_reset()
def saveModel(self, jsonInput):
	self.save()
```
Метод setFilepath устанавливает значение переменной filepath, которая содержит путь к папке, куда должны сохраняться файлы с весами моделей и другими параметрами (и откуда они должны загружаться).

Метод set_id устанавливает значение player_id, которое нужно для загрузки и сохранения весов. Здесь же вызывается load.

Метод soft_reset вызывается по окончании игры (эпизода обучения) для того, чтобы модель могла быть использована для следующей игры.

Методы load и save загружают и сохраняют веса модели, а также такие параметры как текущие значения переменных eps, beta, update_count и train_count. Названия файлов зависят от площади игрового поля и id игрока.

<a name="aipawn"></a>
### 2.2 AIPawn

Представляет собой компьютерного игрока. Содержит UnitsComponent и реализует интерфейс IsPlayer. Помимо этого, содержит TensorFlowComponent (см. [TensorFlow-Unreal](https://github.com/getnamo/TensorFlow-Unreal?tab=readme-ov-file#blueprint-api)) под названием «QNetwork».

Ход компьютерного игрока определяется следующим образом. Выбирается первый юнит, затем для него генерируются возможные действия, затем эти действия, текущее состояние и награда в формате json отправляются в модуль (вызывается метод [SendJsonInput](https://github.com/getnamo/TensorFlow-Unreal?tab=readme-ov-file#basic-json-string) компонента QNetwork, что в свою очередь вызывает onJsonInput модуля network).

<img width="1154" height="183" alt="5_turn_event" src="https://github.com/user-attachments/assets/333c4b04-12a8-47db-ba18-b7cca5784e58" />
<img width="1142" height="208" alt="6_select_unit_event" src="https://github.com/user-attachments/assets/6d526a0d-47e3-47ef-8db5-48cbf60170ba" />
<img width="958" height="171" alt="7_send_input_event" src="https://github.com/user-attachments/assets/f04c7166-36c2-4ed5-85fb-bdf6991a7a41" />

В ответ из модуля приходит индекс действия, это действие совершается, затем выбирается следующий юнит. Если все юниты совершили какое-то действие, ход передаётся следующему игроку. То есть в модуле происходит только выбор из возможных действий, которые были для него определены игрой, что делает его более универсальным, так как в нём не прописана игровая логика.

<img width="1327" height="195" alt="8_on_input_event" src="https://github.com/user-attachments/assets/3c1bb638-832f-4708-93d4-bc5d93710817" />
<img width="1359" height="430" alt="9_next_unit_event" src="https://github.com/user-attachments/assets/198edde2-6eba-4912-bf76-47bdf0b03b4d" />

В классе AIPawn также определены события Win и LoseCondition, которые вызываются в случае выигрыша или проигрыша соответственно. Оба события так же вызывают метод SendJsonInput, что вызывает в модуле функцию update, так как в этих случаях количество возможных действий равняется нулю.

Наконец, в классе реализованы следующие несколько событий:
* BeginPlay. Вызывается в начале игры. Здесь в QNetwork устанавливается путь к сохранениям (setFilepath) и задаётся идентификатор игрока (set_id), что в свою очередь косвенно запускает загрузку параметров и весов, если они были ранее сохранены.
* Reset. Вызывается при начале новой итерации. Здесь удаляются и затем заново добавляются на стартовые позиции юниты. Затем в QNetwork вызывается softReset.
* EndPlay. Вызывается в конце игры. Здесь в QNetwork вызывается функция сохранения saveModel.

В событиях, перечисленных выше, для вызова функций модуля используется метод [CallCustomFunction](https://github.com/getnamo/TensorFlow-Unreal?tab=readme-ov-file#custom-functions) компонента QNetwork.

<a name="use"></a>
## 3. Использование приложения

При запуске приложения в первую очередь загружается главное меню, где пользователь доступны следующие опции:
* Play. Запуск игры.
* Settings. Эта опция открывает меню настроек.
* Exit. Выход из программы.

<img width="385" height="347" alt="10_main_menu" src="https://github.com/user-attachments/assets/f97a7e6c-b7b1-4c91-a0a7-194c28b6af94" />

В меню настроек пользователь может настроить параметры запуска. В выпадающем списке можно выбрать конфигурацию игроков, с которыми запустится игра:
* Два человеческих игрока.
* Один человеческий игрок и один компьютерный игрок.
* Два компьютерных игрока.

<img width="651" height="300" alt="11_options_menu" src="https://github.com/user-attachments/assets/7482be82-5ede-4d0b-8443-4bd8797b2915" />

Также пользователь может определить количество итераций, как можно увидеть ниже. Чтобы выбранные параметры отобразились при запуске игры, их необходимо сохранить. Иначе, будут использованы настройки по умолчанию.

<img width="633" height="293" alt="12_default_settings" src="https://github.com/user-attachments/assets/0fde9ca0-cb93-4943-9773-8a822fa7946c" />

В начале игры создаются два игрока и для каждого их них добавляется некоторое количество юнитов, которые размещаются на противоположных сторонах игрового поля. Право на первый ход определяется случайным образом. 

<img width="1574" height="880" alt="13_game_start" src="https://github.com/user-attachments/assets/d0ae55dc-e1f6-465e-92d3-730df9cd0357" />

Игрок-человек может выбирать своих юнитов, кликая клетки, на которых они находятся. При этом будут подсвечены все достижимые этим юнитом клетки.

<img width="1573" height="879" alt="14_choosing_unit" src="https://github.com/user-attachments/assets/78c219b1-a822-4c0e-b133-7a6a2ccd95a9" />

Если игрок кликнет на одну из подсвеченных свободных клеток, юнит переместится на неё, при этом в зависимости от удалённости ячейки от юнита, из его очков передвижения будет вычтена соответствующая цена.

<img width="1576" height="877" alt="15_moving_unit" src="https://github.com/user-attachments/assets/72ecdace-9d0b-4f45-88f8-9cb4b55d4800" />

Если в достижимой юнитом зоне находится один из вражеских юнитов, игрок может дать юниту команду его атаковать. Для этого игроку нужно кликнуть клетку с вражеским юнитом.

<img width="1572" height="880" alt="16_enemies_in_range" src="https://github.com/user-attachments/assets/6a4b1711-92cd-48fe-9858-9d5d00193303" />

Если выбранный юнит не находится на соседней с целью клетке, то перед тем как атаковать, он передвинется на ближайшую к нему клетку, из которой он сможет атаковать врага. После чего юнит нанесёт удар по врагу. Если атака не смогла убить цель, юнит останется на месте.

<img width="1573" height="876" alt="17_enemy_alive" src="https://github.com/user-attachments/assets/8669e63c-f376-4996-8727-4f3fd650ca6d" />

В ином случае юнит переместится на клетку, где раньше располагался противник. При любом исходе атака затрачивает все очки передвижения юнита.

<img width="1580" height="877" alt="18_enemy_dead" src="https://github.com/user-attachments/assets/edf9b7ae-cead-4264-9af6-1f3bbca6cc95" />

Нажатием на клавишу Enter игрок передаёт ход следующему игроку. Ход текущего игрока отображается в верхнем левом углу экрана. В начале каждого хода обновляются очки передвижения юнитов.

<img width="1575" height="879" alt="19_passing_turn" src="https://github.com/user-attachments/assets/e961d211-218c-4949-8910-b098b6161038" />

При потере всех своих юнитов игрок выбывает из текущей игры. В свою очередь, последний выживший игрок становится победителем, после чего игровая партия завершается, и сообщение о победителе выводится на экран. Если это была не последняя итерация, то происходит ресет: игроки возвращаются на исходные позиции, и начинается новая игровая партия. Иначе, на экране появляется кнопка возвращения в главное меню, а результаты всех партий сохраняются в отдельный файл с расширением json.

<img width="1574" height="879" alt="20_victory_screen" src="https://github.com/user-attachments/assets/c8466176-5839-49f4-a26c-b663389fccaa" />

Что касается обучения, для его запуска достаточно установить режим «Two Computer Players» в меню настроек и задать количество итераций. 

<img width="623" height="276" alt="21_learning_settings" src="https://github.com/user-attachments/assets/63c07bca-7e76-465c-9842-f30b14c91411" />

Результаты для приведённых выше настроек представлены на рисунке ниже. Согласно им, игрок 1 выиграл 26 раз, а игрок 2 – 24 раза.

<img width="454" height="128" alt="22_results" src="https://github.com/user-attachments/assets/d8bd4019-709f-43cd-bcfe-990e39394348" />
